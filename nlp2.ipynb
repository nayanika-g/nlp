{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/gracegupta/Downloads/project final with type 2_all unique_1_31_2019 (1).csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>activity</th>\n",
       "      <th>administering_ic</th>\n",
       "      <th>application_id</th>\n",
       "      <th>application_type</th>\n",
       "      <th>arra_funded</th>\n",
       "      <th>award_notice_date</th>\n",
       "      <th>budget_end</th>\n",
       "      <th>budget_start</th>\n",
       "      <th>direct_cost_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>project_start</th>\n",
       "      <th>study_section</th>\n",
       "      <th>study_section_name</th>\n",
       "      <th>subproject_id</th>\n",
       "      <th>suffix</th>\n",
       "      <th>support_year</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>total_cost_sub_project</th>\n",
       "      <th>abstract_text</th>\n",
       "      <th>uni_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A03</td>\n",
       "      <td>AH</td>\n",
       "      <td>2056338</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1994-07-01</td>\n",
       "      <td>1995-06-30</td>\n",
       "      <td>1994-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1994-07-01</td>\n",
       "      <td>NSS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A03</td>\n",
       "      <td>AH</td>\n",
       "      <td>2056372</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995-05-19</td>\n",
       "      <td>1996-06-30</td>\n",
       "      <td>1995-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-07-01</td>\n",
       "      <td>NSS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id activity administering_ic  application_id  application_type  \\\n",
       "0   0      A03               AH         2056338                 1   \n",
       "1   0      A03               AH         2056372                 1   \n",
       "\n",
       "   arra_funded award_notice_date  budget_end budget_start  direct_cost_amt  \\\n",
       "0          NaN        1994-07-01  1995-06-30   1994-07-01              NaN   \n",
       "1          NaN        1995-05-19  1996-06-30   1995-07-01              NaN   \n",
       "\n",
       "    ...    project_start  study_section  study_section_name  subproject_id  \\\n",
       "0   ...       1994-07-01            NSS                 NaN            NaN   \n",
       "1   ...       1995-07-01            NSS                 NaN            NaN   \n",
       "\n",
       "  suffix  support_year  total_cost total_cost_sub_project abstract_text  \\\n",
       "0    NaN             1         NaN                    NaN           NaN   \n",
       "1    NaN             1         NaN                    NaN           NaN   \n",
       "\n",
       "  uni_type  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first few rows of the CSV file\n",
    "pd.read_csv(filepath, nrows=2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = []\n",
    "chunksize = 10 ** 6\n",
    "for chunk in pd.read_csv(filepath, chunksize=chunksize):\n",
    "    appended_data.append(chunk[['abstract_text', 'uni_type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = pd.concat(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  abstract_text uni_type\n",
      "0           NaN      NaN\n",
      "1           NaN      NaN\n",
      "2           NaN      NaN\n",
      "3           NaN      NaN\n",
      "4           NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "print(appended_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2458227, 2)\n"
     ]
    }
   ],
   "source": [
    "print(appended_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all abstracts from R1 and R2 schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "isR1 = appended_data['uni_type'] == 'R1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        abstract_text uni_type\n",
      "20     DESCRIPTION (provided by applicant):    The...       R1\n",
      "39  Project 2 - Project Summary/Abstract The proje...       R1\n",
      "45  Abstract/Summary (Administrative Core; Core 1)...       R1\n",
      "46  The analysis and visualization of high field m...       R1\n",
      "49  Invasive cervical cancer (ICC) is the most com...       R1\n"
     ]
    }
   ],
   "source": [
    "print(appended_data[isR1].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250497, 2)\n"
     ]
    }
   ],
   "source": [
    "print(appended_data[isR1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "isR2 = appended_data['uni_type'] == 'R2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         abstract_text uni_type\n",
      "218  ?    DESCRIPTION (provided by applicant): Exec...       R2\n",
      "322  PROJECT SUMMARY (See instructions):  African-A...       R2\n",
      "327  To understand how signaling proteins function,...       R2\n",
      "358  Innate immunity is an ancient system that prev...       R2\n",
      "589  PROJECT SUMMARY  Fibrolamellar hepatocellular ...       R2\n"
     ]
    }
   ],
   "source": [
    "print(appended_data[isR2].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22241, 2)\n"
     ]
    }
   ],
   "source": [
    "print(appended_data[isR2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cancer-only abstracts for R1 and R2 schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_R2 = appended_data[isR2]['abstract_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_R2 = abstracts_R2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218    ?    DESCRIPTION (provided by applicant): Exec...\n",
      "Name: abstract_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(abstracts_R2[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_abstracts_R2 = []\n",
    "for doc in abstracts_R2:\n",
    "    if 'cancer' in doc:\n",
    "        cancer_abstracts_R2.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2238\n"
     ]
    }
   ],
   "source": [
    "print(len(cancer_abstracts_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To understand how signaling proteins function, it is crucial to know the timeordered sequence of events that lead to the signaling state. When the messenger is chemical, the time required to diffuse to and bind in the active site of a signaling protein is typically far longer than the timescale for protein conformational change [1]. For the structural determination of the kinetics of enzymatic reactions we will focus on small GTPases and their co-enzymes. Small GTPases are molecular switches that cycle between a GTP-bound active and a GDP-bound inactive form. The switch is catalyzed by Guanine nucleotide Exchange Factors (GEFs) and GTPase-Activating Proteins (GAPs), the latter catalyze the hydrolysis of GTP to GDP to deactivate the small GTPase. This system is of very high, general importance in cell biology with particular impact on disease processes, especially cancer, but also several infectious diseases. For proof-ofprinciple, we chose the Arl3-RP2 complex as GTPase-GAP pair [2]. The gene encoding for the GAP protein RP2 (Retinitis pigmentosa 2) is highly mutated in patients of X-linked Retinitis pigmentosa, with mutational hotspots in residues catalyzing the GAP reaction on Arl3. Retinitis pigmentosa refers to a heterogeneous group of inherited ocular diseases that result in a progressive retinal degeneration affecting 1 in 3,000?5,000 people.']\n"
     ]
    }
   ],
   "source": [
    "print(cancer_abstracts_R2[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_R1 = appended_data[isR1]['abstract_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_R1 = abstracts_R1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20       DESCRIPTION (provided by applicant):    The...\n",
      "Name: abstract_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(abstracts_R1[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_abstracts_R1 = []\n",
    "for doc in abstracts_R1:\n",
    "    if 'cancer' in doc:\n",
    "        cancer_abstracts_R1.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36128\n"
     ]
    }
   ],
   "source": [
    "print(len(cancer_abstracts_R1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['   DESCRIPTION (provided by applicant):    The major goal of the Biomedical Research Tower (BRT) is to create a multidisciplinary biomedical research and education center for The Ohio State University Medical Center (OSUMC) that will be a centerpiece of a dramatically enhanced health sciences campus. Integral to the University\\'s Academic Plan for becoming a top public research institution, the BRT will greatly advance the academic mission of the University while bringing enormous value in improved health care, advanced technology, and economic growth to the State of Ohio. The specific renovation project being proposed is for the \"buildout\" of a floor of the BRT (approximately 24,000 asf) to centralize and support one of the fastest growing and developing areas of cancer research, the Experimental Therapeutics Program (ETP) of The Ohio State University Comprehensive Cancer Center (OSUCCC).       The ETP plays a critical role in the discovery and development of new cancer therapies. A strong tradition of therapeutic research has been the focus of our nationally recognized program enabling both the design and conduct of trials at the initial interface between basic and clinical research for \"first in human\" studies. The explosion in knowledge of molecular genetics has had a profound impact on understanding both efficacy and toxicity assessments in patients on experimental clinical trials. These remarkable advances have paved the way for incorporating pharmacodynamics, pharmacokinetics, and pharmacogenomics investigations into the evaluation of these newer therapies. Validation of the scientific \"proof of principle\" underlying each new therapeutic approach has been mandated. The requested federal funding will enable our institution to bring highly trained experts in drug discovery and development into close proximity, and thus improve the scientific process of discovering new therapeutic approaches for patients with cancer.       Furthermore, the proposed construction of a dedicated floor for experimental therapeutics within the BRT will facilitate a planned expansion of faculty recruitment targeting specific areas of solid tumor oncology. Our Comprehensive Cancer Center in conjunction with the College of Medicine has outlined a plan for recruitment of at least twelve new investigators. The specific focus of the recruitments will include: a Director of Gastrointestinal Malignancy;a Director of Head and Neck Medical Oncology;a senior physician scientist with expertise in multiple myeloma;three junior faculty investigators in solid tumor oncology;and two additional clinical investigators in hematological malignancies (leukemia and lymphoma). In addition, the recruitment plan calls for three additional basic scientists (PhD\\'s) with a senior basic scientist to direct and coordinate phase I activities in the James Cancer Hospital. Several of these individuals have already been identified, and have just re-located to The Ohio State University. The success in recruitment will necessitate identification of new space because the existing peer reviewed research funding of the established investigators also continues to grow. Thus, the expansion of new space in the BRT will both enhance research productivity, and allow the full recruitment plan to be executed over the next three years. While the initial plans call for the recruitment of specific investigators into Oncology, there is a long-term initiative, OSU Project Cancer, designed to support a larger expansion, including the recruitment of 90 individuals, with a major emphasis being the development of Solid Tumor Oncology.       The development of the ETP at Ohio State will enhance access to outstanding patient care, promising and innovative research protocols, and exemplary teaching. Recruiting top caliber faculty to fill voids in the Division of Hematology and Medical Oncology in the areas of head and neck cancer, lung cancer, gastrointestinal cancers, and genitourinary malignancies will enable our physicians to provide disease specific expertise in therapeutics that will have a national impact on fatal diseases. In this manner, Ohio State will also be a beacon for training the next generation of scientists committed to experimental therapeutics. This cadre of multi-disciplinary experts will spread the scientific approach for translational research in cancer treatment. With a significant academic list of accomplishments in a focused specialty, our cancer program will be recognized as one of the ten best in the United States.        ', 'Invasive cervical cancer (ICC) is the most common cause of cancer death worldwide, and while decreasing in prevalence in the majority of the developed world, this disease continues to disproportionately affect certain populations in the United States (US). In particular, the rate of ICC incidence and mortality in Appalachian women is the highest in the US. While many risk factors are known to influence ICC development, little is known about the role of hereditary and genetic susceptibility factors. The transforming growth factor beta (TGF-B) is a universal critical regulator of various cellular functions, including cell proliferation, via its binding with a receptors complex on the cell surface. Cancer cells frequently avoid the inhibitory influence of TGF-B on cell proliferation via somatic inactivation of key components of the signaling pathway, including the ligand and receptor complex. Additionally, germline polymorphisms in the ligand and receptors have been associated with cancer development and increased cancer susceptibility. In this proposal, we hypothesize that the increased incidence of ICC observed in Appalachian women over their non-Appalachian counterparts is due in part to inherited and somatic alteration of the TGF-B ligand and receptor complex that can be further potentiated in association with various environmental, behavioral, and socioeconomic risk factors. Specifically, we will determine prevalence of inherited polymorphic and somatically acquired variants of key TGF-B pathway components in a large cohort of Appalachian ICC patients compared to healthy Appalachian women. Furthermore, we will determine whether these genetic alterations contribute individually or in combination with other known environmental (Human Papillomavirus, Epstein-Barr Virus), behavioral (smoking), and social (stress, social networks) risk factors, to the increased susceptibility of Appalachian women to ICC  development.', '?     DESCRIPTION (provided by applicant): We propose to continue our study of the health effects of metalworking fluids (MWF) in a cohort of Michigan autoworkers by focusing on cancer incidence and applying the advanced statistical methods necessary to characterize exposure-response relations without bias due to the healthy worker survivor effect (HWSE). Though we have reported strong evidence that straight MWF exposure increases the risk of bladder cancer and melanoma skin cancer, anticipated links with respiratory, digestive and other cancers have been less clear. We now have substantial experience applying causal g-methods to control for HWSE in mortality studies of this cohort - from the first implementation of g-estimation in an occupational study, to our recent incorporation of quantitative exposure into this approach. In contrast with common perceptions about HWSE, our previous work suggests that the bias can affect studies of cancer as well as other chronic diseases. Whereas mortality follow-up began in 1941 for the original UAW-GM cohort, cancer incidence cannot be observed until decades later when two cancer registries were initiated - the Detroit SEER registry in 1973 and Michigan Cancer registry in 1985. Thus the sub-cohort of 37,786 workers eligible for this proposed cancer study includes only those who survived until the later start of follow-up, and so is left truncated If, as we have shown, MWF exposure increases mortality risk, then the cancer incidence cohort will likely include fewer subjects susceptible to the health effects of MWF exposure. We propose to apply a novel approach based on inverse weighting to address left truncation, as an additional source of healthy worker survivor bias, in this proposed study of long-term exposure to MWF in relation to risk of selected cancers. MWF are complex mixtures of oils and chemical additives widely used to cool and lubricate metal machining operations. The airborne particulate matter (PM) generated when MWF are sprayed, shares common components with traffic related PM that have been linked to lung cancer, but occurs at up to two orders of magnitude higher than ambient levels. Thus MWF pose a potential hazard to millions of workers in automobile manufacturing, specialty metals and other metal machining jobs related to electronics manufacturing, new technologies, or alternative energy. In 2010, the United Autoworkers again unsuccessfully petitioned OSHA for a MWF standard primarily on the basis of asthma and hypersensitivity pneumonitis. The goal of our proposed study, to clarify the cancer risk - both in terms of etiology and risk assessment - may be necessary spur prevention activities.', 'DESCRIPTION (provided by applicant): This application proposes that Yale University become a Network Lead Academic Participating Site (LAPS) under the newly established NCI National Clinical Trials Network. As an LAPS Yale University will 1. Promote and advocate for NCTN clinical research at the Yale Comprehensive Cancer Center (YCC); 2.continue expanding accrual to NCTN trials and provide broad based access to these important national trials to YCC membership; 3. Provide the scientific leadership necessary in order to advance the cause of NCI funded research in the NCTN system and lead important trials that will advance cancer treatment and standard of care; 4. Provide the translational science and laboratory support for these NCTN clinical trials; 5. Train and mentor Young Investigators in the NCTN groups and help them to provide the next generation of scientific leadership', 'Abstract Differentiation of CD4 T cells into functionally distinct effector subsets has been intensively studied in vitro and the subset-specific roles of CD4 T cells have been also considered important in vivo. TH1 CD4 T cells, which express T-bet and IFN-?, are thought to be important for both adaptive and innate immune responses to infection by intracellular pathogens and cancers. However, since T-bet is expressed in both CD4 and CD8 T cells, and IFN-? is expressed by other cell populations, such as CD8 T cells and NK cells, the conventional conditional knockout or germline knockout approaches have failed to specifically demonstrate the requirement for TH1 cells in vivo. To address this fundamental question, we have developed a new genetic tool and will define the roles of TH1 cells in vivo using viral infection models.', 'PROJECT SUMMARY Innate lymphoid cells (ILCs) are a recently described class of lymphocytes that are critical for defense against a variety of pathogens, and their dysfunction has been associated with several inflammatory pathologies. Thus, understanding how ILCs are regulated is key to developing treatments for a wide array of infectious and inflammatory diseases. ILCs have been divided into three groups (ILC1, ILC2 and ILC3) based on their cytokine secretion profiles and functions. Recently, multiple factors have been identified to play critical roles in the development or function of ILCs; yet, most of these factors are functionally relevant in more than one group of ILCs. These results suggest that additional regulatory mechanisms unique to each ILC group might exist and that they could be critical for the functional differentiation of the different ILC subsets. Addressing this gap of knowledge will not only lead to a better understanding of the pathways that control the development, homeostasis and functions of each ILC group, but it might also uncover new antimicrobial or immunomodulatory therapeutic targets. Long noncoding RNAs (lncRNAs) are a novel class of noncoding RNAs that are now considered key regulators of gene expression programs. Importantly, lncRNAs are expressed in a more cell type-specific manner than protein coding genes, suggesting that they might play central roles in establishing gene expression programs that are unique to specific cell populations in vivo. However, whether lncRNAs can regulate the functional specification of individual ILC groups has not yet been examined. Thus, we hypothesize that there are lncRNAs that are specifically expressed in each group of ILCs and that they play critical roles in establishing the transcriptional programs unique to each ILC subset. To address this hypothesis, we combined a novel bioinformatic workflow to identify lncRNAs expressed in each ILC subset with the CRISPR/Cas9 system to rapidly generate knockout mouse lines. In our preliminary studies, we identified a lncRNA (termed Rroid) that is specifically expressed in mouse and human ILC1s, but not in ILC2s or ILC3s. Importantly, ablation of Rroid in the mouse leads to profound defects in the numbers and function of ILC1s. Furthermore, we found that Rroid controls the expression of the transcriptional repressor Id2 in ILC1s and that dysregulation of Id2 is the main cause for ILC1 dysfunction in Rroid-deficient mice. In here, we propose (i) to determine what aspects of ILC1 development, maturation or function Rroid regulates (Aim 1) and (ii) to establish the molecular mechanism by which Rroid regulates Id2 expression in ILC1s (Aim 2). Altogether, the completion of the proposed studies will fundamentally advance our knowledge of lncRNA regulation of the immune system and define the role of a novel lncRNA in ILC1s. Furthermore, since ILC1s are essential for cancer immunosurveillance and defense against intracellular pathogens, Rroid might be a novel therapeutic target for the treatment of these prevalent diseases. Finally, we will establish all the technical elements for future efforts that will aim to identify highly relevant lncRNAs in each of the ILC subsets.', 'PROJECT SUMMARY With the growing emphases and clinical needs for molecular and genetic based diagnosis and treatment monitoring, both academic programs and scanner manufactures are making significant effort to improve and innovate magnetic resonance imaging (MRI) and spectroscopy (MRS) methods for interrogating metabolic functions and abnormalities in patients. Diffuse infiltrating low grade gliomas (LGG) are among the most lethal cancers and present great challenges in diagnosis and treatment, as they vary considerably in morphology, location, genetic alterations and response to therapy. One major challenge in the clinical management of LGG patients is the lack of accurate and non-invasive methods to stratify patients by tumor subtype to help with early diagnosis, individualized treatment and longitudinal clinical management. The discovery of heterozygous mutations in the isocitrate dehydrogenase (IDH1 and IDH2) genes in up to 70 % of LGG and secondary glioblastoma multiforme (GBM) links genetic alterations to tumor metabolism, prognosis and responses to treatment. IDH mutations lead to a neofunction of catalyzing the NADP-dependent reduction of ?-ketoglutarate to oncometabolite R(-)-2-hydroxyglutarate, (2-HG). These important findings are changing the clinical paradigm of managing gliomas, if given the ability of detecting and measuring 2-HG to identify IDH mutation-bearing tumors, predict the prognosis, define subset-specific treatments and monitor therapeutic response. Our early study found the unique MR spectroscopic signature of 2-HG, which allowed us to establish 2-HG as a biomarker of IDH mutations with potential to quantify 2-HG in brain tumor patients using a spatially localized two-dimensional (2D) correlation spectroscopy (COSY) MRS method. This new strategy for detecting 2-HG can overcome the limitation of conventional 1D J-coupling editing MRS approaches in resolving 2-HG and other metabolites of interest. Investigators at Emory University in partnership with engineers from Siemens MR R&D are proposing to develop and implement a 2D COSY platform on the clinical scanner for non-invasive genetically classifying and characterizing gliomas as well as potentially monitoring retreatment responses and tumor progression in LGG patients using 2-HG as a biomarker. This project combines complementary strengths, expertise and resources of Emory and Siemens with a translational goal to integrate the latest cancer biology and biomarker research and advanced imaging technology to provide a paradigm shifting imaging tool for clinical management of brain tumors. The specific aims are: 1) to develop a sensitivity enhancing strategy with the latest 64-channel coil array and a novel coil proximity weighted parallel MR spectroscopy data acquisition and combination for 2D COSY; 2) to test, optimize and implement the developed method and to develop in-line data processing toolbox for processing and analyzing 2D COSY data on the clinical scanner; and 3) to test and validate the developed 2D COSY platform for genetically profiling gliomas using 2-HG as a marker and detecting other metabolites in patients.', '  [unreadable] DESCRIPTION (provided by applicant):  Protein kinases mediate many cell signaling events, and their tight control is essential for regulating essential processes ranging from cell division to energy metabolism. Thus, it is not surprising that protein kinases are directly or indirectly involved in many diseases and that kinases are key drug targets. For example, Src kinase was the first identified proto-oncogene and the formation of a de-regulated Abl fusion protein (BCR-Abl) is the cause of disease in 95% of patients with chronic myeloid leukemia. X-ray crystal structures have shown that the same kinases can obtain an active and various inactive conformations, implying that kinases are inherently flexible. How the active and inactive states are stabilized and how the states interconvert are key questions in understanding kinase regulation. Because X-ray crystal structures provide only static snapshots, we will use nuclear magnetic resonance (NMR) experiments to study the time scales and amplitudes of structural interconversions in Abl and Src kinase domains.  [unreadable]   [unreadable] BCR-Abl is the target of the clinically highly successful drug imatinib (Gleevec(r), Novartis) in the treatment of chronic myelogenous leukemia (CML). Why does imatinib bind and inhibit c-Abl but not the structurally closely related c-Src kinase? The crystal structure of Src in complex with imatinib shows protein-drug interactions similar to that of Abl, even though the affinity of imatinib for Src is orders of magnitude lower than for Abl. Because imatinib binds only to the inactive conformation of the kinase, drug binding is intimately related to the interconversion between active and inactive states. The goal of this study is to examine whether differences in this interconversion underlie the differential sensitivities for imatinib. Therefore, we will compare the time scales and amplitudes of backbone motions between Src and Abl kinases in the presence of imatinib by NMR experiments. In preparation for these dynamics experiments, we have established expression systems and NMR conditions and will next pursue the assignment of the Src and Abl NMR spectra.  [unreadable]   [unreadable] Kinase inhibitory drugs such as imatinib have a great therapeutic potential because of the many signaling events that protein kinases mediate. However, these drugs have to be exceptionally specific for their target kinase and resistance mutations can render these drugs ineffective as seen in leukemia patients under imatinib treatment. The proposed experiments will clarify how inhibitors such as imatinib exploit the characteristic movements of kinase proteins, rather than just their structures, to achieve specificity. Furthermore, the results will have broader impact on the understanding of the fundamental mechanisms of kinase regulation and of drug resistance mutations that are known to arise in cancer patients undergoing kinase inhibitory treatment.  [unreadable]     [unreadable] [unreadable] ', 'DESCRIPTION (provided by applicant):  The high mortality of ovarian cancer (OvCa) is caused by the wide dissemination of cancer cells within the abdominal cavity, which results in significant tumor burden. The first cells which metastatic OvCa cells encounter are the mesothelial cells which line the peritoneum and omentum of the abdominal cavity. We established an organotypic 3D culture of the peritoneum and omentum using primary human fibroblasts, mesothelial, and OvCa cells that allowed us to mimic the early steps of metastasis in a cell culture. Following the attachment of OvCa cells to the peritoneum, cancer cell derived MMP-2 cleaves fibronectin into smaller pieces which promotes ????-integrin mediated adhesion and metastasis. c-Met supports the metastasis of OvCa cells through its physical with ????- integrin, which allows the HGF-independent activation of c-Met/Src/Fak intracellular signaling. Additional mechanisms which promote OvCa dissemination involve the interaction of ????-integrin with the urokinase receptor (uPAR) Most of the results obtained during the previous funding period point towards a prominent role for the ????-integrin - fibronectin interaction in ovarian cancer metastasis. This is also supported by our clinical studies, which established that 90% of all OvCa metastases express fibronectin in the stroma. Because our preliminary data suggest that mesothelial cells are the source of fibronectin, the primary hypothesis underlying this application is that OvCa cells stimulate human mesothelial cells to secrete fibronectin, which increases OvCa invasion and metastasis. In Aim I we will elucidate the mechanisms with which the interaction of mesothelial cells with OvCa cells promotes invasion and metastasis through secretion of fibronectin. Experiments will include studying the role of fibronectin in vivo using a floxed fibronectin knock-out mouse which allows deletion of fibronectin in mesothelial cells, analysis of fibronectin promoter regulation in mesothelial cells,  and a siRNA screen to understand how OvCa cells promote fibronectin secretion in the mesothelial cells. In Aim II we will perform a detailed study to identify which fibronectin fragments promote the association between ????-integrin and c-Met. This will be followed by deletion of functional domains in ??-integrin and c-Met to define the binding sites where the two proteins interact and the biological significance of this protein-protein interaction for the intra  cellular signaling, invasion, and metastasis of OvCa cells. Based on the findings in Aims I and II,  in Aim III, we will combine a c-Met inhibitor with a fibronectin peptide known to block the effect of fibronectin binding integrins and test them in pre-clinical treatment studies using an orthotopi xenograft model (bursal injection of primary cells) as well as in a genetic model of OvCa. Successful inhibition of fibronectin function could result in a novel and clinically relevant approach to the treatment of metastatic ovarian cancer.          PUBLIC HEALTH RELEVANCE:  The biology of ovarian cancer metastasis is still poorly understood and, therefore, minimal progress has been made in its treatment. Our findings, that the mesothelial cells lining the abdominal cavity promote ovarian cancer metastasis through the secretion of fibronectin, have led us to examine how the interaction between fibronectin and its receptor, ????-integrin, promotes tumor growth and metastasis. Studies will include pre-clinical testing of compounds that inhibit this interaction, with the goal of translating the knowledge gained into new treatments for this devastating disease.', 'DESCRIPTION (provided by applicant): In this revised Program Project renewal application, the applicant group is representative of three primary institutions. The Ohio State University (OSU), the University of Illinois at Chicago (UIC), and the University of North Carolina at Greensboro (UNCG). The participants have combined their vast experience in the isolation, structure elucidation, and biological evaluation of natural products, to continue the development of a consolidated, integrated program for the discovery of novel anticancer agents of diverse origin for development as cancer chemotherapeutic agents. Plant materials to be studied in Project 1 (OSU) will be collected by established botanists located in tropical countries with the assistance of the NAPRALERT database (Project 2; UIC), and cyanobacteria (Project 2) and filamentous fungi (Project 3 UNCG) will also be accessed. Organisms acquired will be extracted and evaluated in a diverse battery of relevant mechanism-based, cell-based, and tumor-growth related assays currently operational at OSU (Project 1), UIC (Core A), Columbia University (through Project 3), and a new pharmaceutical company partner, Eisai Inc., Andover, MA (through Core C at OSU). Dereplication of known active compounds will be accomplished at OSU, UIC, and UNCG using computerized literature surveys and LC-MS coupled to bioassays. Bioassay-directed fractionation will be employed in Projects 1-3, for the elucidation of active principles. Lead development of active natural products via medicinal chemistry and pharmacokineticsrelated studies will be conducted at OSU (Core B), facilitated by the OSU Biostatistics group (Core C). Novel, active compounds thus discovered will be further evaluated in our panel of in vitro and in vivo bioassays (Projects 1 and 3, Cores A and Eisai). Group decisions will be made regarding the further development of agents for potential use as anticancer agents. The more advanced states of biological and toxicological testing, as well as the procurement of larger quantities of lead compounds will be sponsored by Eisai (through Core C). The Consortium will work with the involvement of the NCI Program Director in the discovery process, and plans to hold regular meetings of key scientific personnel (inclusive of our External and OSU Internal Advisory panels) to enhance communication and decision-making processes, to be organized by Core C. Excellent facilities for the isolation, structure determination, chemical modification, synthesis, and in vitro and in vivo biological evaluation, and overall project data management are available.    RELEVANCE (See instructions): Cancer is responsible for about one in every four deaths in the United States, and new treatments are urgently needed. It is the overall goal of the integrated studies in this renewal application to discover novel chemicals from selected tropical rainforest plants, as well as cyanobacteria and fungi, for development as cancer chemotherapeutic agents, particularly for tumors not cured by present treatment methods.']\n"
     ]
    }
   ],
   "source": [
    "print(cancer_abstracts_R1[:10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gracegupta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gracegupta/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gracegupta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'<title>(.*)</title>', re.UNICODE)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.compile('<title>(.*)</title>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    #tokenize into words\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    #remove stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english') + ['reuter', '\\x03'])\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    #lower capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    #lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_abstracts_R1 = []\n",
    "for item in cancer_abstracts_R1:\n",
    "    processed_abstracts_R1.append(preprocessing(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_abstracts_R2 = []\n",
    "for item in cancer_abstracts_R2:\n",
    "    processed_abstracts_R2.append(preprocessing(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cancer', 153103),\n",
       " ('cell', 111942),\n",
       " ('the', 99742),\n",
       " ('research', 63670),\n",
       " ('study', 57103),\n",
       " ('tumor', 53767),\n",
       " ('aim', 42709),\n",
       " ('protein', 38524),\n",
       " ('clinical', 37077),\n",
       " ('we', 36015),\n",
       " ('specific', 35479),\n",
       " ('mechanism', 34077),\n",
       " ('gene', 33924),\n",
       " ('in', 33666),\n",
       " ('human', 31612),\n",
       " ('program', 31480),\n",
       " ('patient', 30986),\n",
       " ('new', 30946),\n",
       " ('development', 30365),\n",
       " ('disease', 29152),\n",
       " ('this', 28929),\n",
       " ('project', 28759),\n",
       " ('model', 27535),\n",
       " ('role', 25809),\n",
       " ('treatment', 24409),\n",
       " ('function', 24177),\n",
       " ('data', 24166),\n",
       " ('provided', 23840),\n",
       " ('molecular', 23342),\n",
       " ('breast', 22744),\n",
       " ('dna', 22401),\n",
       " ('also', 22345),\n",
       " ('determine', 22215),\n",
       " ('applicant', 22108),\n",
       " ('description', 22077),\n",
       " ('pathway', 21893),\n",
       " ('using', 21393),\n",
       " ('activity', 21054),\n",
       " ('provide', 20899),\n",
       " ('novel', 20861)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(processed_abstracts_R1, n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cell', 8756),\n",
       " ('cancer', 7847),\n",
       " ('the', 5775),\n",
       " ('study', 3580),\n",
       " ('protein', 3574),\n",
       " ('research', 3370),\n",
       " ('tumor', 2805),\n",
       " ('mechanism', 2554),\n",
       " ('aim', 2541),\n",
       " ('we', 2335),\n",
       " ('specific', 2189),\n",
       " ('disease', 2111),\n",
       " ('gene', 2103),\n",
       " ('in', 2094),\n",
       " ('development', 2080),\n",
       " ('human', 2068),\n",
       " ('dna', 2031),\n",
       " ('role', 2027),\n",
       " ('new', 1856),\n",
       " ('function', 1786),\n",
       " ('activity', 1722),\n",
       " ('this', 1698),\n",
       " ('expression', 1643),\n",
       " ('project', 1636),\n",
       " ('also', 1555),\n",
       " ('pathway', 1544),\n",
       " ('health', 1522),\n",
       " ('molecular', 1478),\n",
       " ('provided', 1469),\n",
       " ('program', 1444),\n",
       " ('patient', 1415),\n",
       " ('factor', 1413),\n",
       " ('unreadable', 1368),\n",
       " ('determine', 1362),\n",
       " ('applicant', 1344),\n",
       " ('proposed', 1339),\n",
       " ('may', 1334),\n",
       " ('description', 1324),\n",
       " ('understanding', 1323),\n",
       " ('model', 1290)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(processed_abstracts_R2, n=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_abstracts_R1 = [doc.split() for doc in processed_abstracts_R1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(\n",
    "        processed_abstracts_R1,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76556"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.wv.vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('researcher', 0.62552410364151),\n",
       " ('scientific', 0.6052536964416504),\n",
       " ('science', 0.5511459112167358)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"research\"\n",
    "model.wv.most_similar(positive=w1, topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('motif', 0.5320680737495422),\n",
       " ('substrate', 0.5166242718696594),\n",
       " ('subunit', 0.5162673592567444)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = \"protein\"\n",
    "model.wv.most_similar(positive=w2, topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pathway', 0.5750380754470825),\n",
       " ('mode', 0.514131486415863),\n",
       " ('basis', 0.5070115327835083)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3 = \"mechanism\"\n",
    "model.wv.most_similar(positive=w3, topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial that didn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on preprocessed R1 cancer abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 300\n",
    "\n",
    "w2v = Word2Vec(processed_abstracts_R1, size=EMB_DIM, window=5, min_count=5, negative=15, iter=10, \n",
    "              workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv #get trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using embeddings in neural model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/gracegupta/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/gracegupta/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "#getting tokenized and part-of-speech tagged data from the corpus\n",
    "train_words = conll2000.tagged_words(\"train.txt\")\n",
    "test_words = conll2000.tagged_words(\"test.txt\")\n",
    "print(train_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_vocabulary(tagged_words):\n",
    "    \"\"\"\n",
    "    Accepts text in the form of (word, pos) tuples and returns\n",
    "    a dictionary mapping POS-tags to unique ids.\n",
    "    \"\"\"\n",
    "    tag2id = {}\n",
    "    for item in tagged_words:\n",
    "        tag = item[1]\n",
    "        tag2id.setdefault(tag,len(tag2id))\n",
    "    return tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the word_vectors.vocab dictionary stores Vocab objects, rather than integers\n",
    "#but we would like our dictionary to map words to ints\n",
    "word2id = {k: v.index for k, v in word_vectors.vocab.items()}\n",
    "tag2id = get_tag_vocabulary(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
    "    \"\"\"\n",
    "    Adds a new word to the existing matrix of word embeddings.\n",
    "    \"\"\"\n",
    "    #inserting the vector before given index, along axis 0.\n",
    "    embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
    "    \n",
    "    #updating the indexes of words that follow the new word.\n",
    "    word2id = {word: (index + 1) if index >= new_index else index\n",
    "              for word, index in word2id.items()}\n",
    "    word2id[new_word] = new_index\n",
    "    return embedding_matrix, word2id\n",
    "\n",
    "UNK_INDEX = 0 \n",
    "UNK_TOKEN = \"UNK\"\n",
    "\n",
    "embedding_matrix = word_vectors.vectors\n",
    "unk_vector = embedding_matrix.mean(0)\n",
    "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector,\n",
    "                                        UNK_INDEX, embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_data(tagged_words, word2id, tag2id):\n",
    "    \"\"\"\n",
    "    Replaces all words and tags with their corresponding ids \n",
    "    and separates words (features) from the tags (labels).\n",
    "    \"\"\"\n",
    "    X, Y = [], [] #X will hold word ids, Y will hold ids of their tags\n",
    "    unk_count = 0 #to keep track of the number of unknown words\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        Y.append(tag2id.get(tag))\n",
    "        if word in word2id:\n",
    "            X.append(word2id.get(word))\n",
    "        else:\n",
    "            X.append(UNK_INDEX)\n",
    "            unk_count += 1\n",
    "    print(\"Data created. Percentage of unknown words: %.3f\" % (unk_count/len(tagged_words)))\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 0.866\n",
      "Data created. Percentage of unknown words: 0.868\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_int_data(train_words, word2id, tag2id)\n",
    "X_test, Y_test = get_int_data(test_words, word2id, tag2id)\n",
    "\n",
    "Y_train, Y_test = to_categorical(Y_train), to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 300)            20700     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 44)                2244      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 44)                0         \n",
      "=================================================================\n",
      "Total params: 37,994\n",
      "Trainable params: 37,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/gracegupta/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "141184/211727 [===================>..........] - ETA: 2s - loss: 2.5290 - acc: 0.2651"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[120,0] = 70 is not in [0, 69)\n\t [[{{node embedding/embedding_lookup}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9b883b01ecc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m              verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[120,0] = 70 is not in [0, 69)\n\t [[{{node embedding/embedding_lookup}}]]"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 50\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def define_model(embedding_matrix, class_count):\n",
    "    \"\"\"\n",
    "    Creates and returns a POS model, which only takes\n",
    "    one word as input.\n",
    "    \"\"\"\n",
    "    vocab_length = len(embedding_matrix)\n",
    "    model = Sequential()\n",
    "    \n",
    "    #A layer which turns word indices into vectors\n",
    "    model.add(Embedding(input_dim=vocab_length,\n",
    "                       output_dim=EMB_DIM,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_SIZE))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "                 loss=\"categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "pos_model = define_model(embedding_matrix, len(tag2id))\n",
    "pos_model.summary()\n",
    "\n",
    "#Training the model\n",
    "pos_model.fit(X_train,\n",
    "             Y_train,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             epochs=1,\n",
    "             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47377/47377 [==============================] - 2s 40us/sample - loss: 2.4906 - acc: 0.26700s - loss: 2.482\n",
      "Accuracy: 0.27\n",
      "Most common errors:\n",
      " [('UNK', 34713), ('$', 5), ('7', 5), (';', 2)]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, id2word, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the given model by computing the accuracy of its predictions\n",
    "    on the given test data and prints out 10 most mistagged words.\n",
    "    \"\"\"\n",
    "    _, acc = model.evaluate(x_test, y_test) #get accuracy of the model\n",
    "    print(\"Accuracy: %.2f\" % acc)\n",
    "    \n",
    "    #the following lines are used to get most commonly mistagged words\n",
    "    y_pred = model.predict_classes(x_test) #get model predictions\n",
    "    error_counter = collections.Counter()\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        correct_tag_id = np.argmax(y_test[i])\n",
    "        if y_pred[i] != correct_tag_id:\n",
    "            word = id2word[x_test[i]]\n",
    "            error_counter[word] += 1\n",
    "    print(\"Most common errors:\\n\", error_counter.most_common(10))\n",
    "    \n",
    "id2word = sorted(word2id, key=word2id.get)\n",
    "evaluate_model(pos_model, id2word, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a context dependent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_INDEX = 1\n",
    "EOS_TOKEN = \"EOS\"\n",
    "\n",
    "#creating a random end-of-sequence vector\n",
    "eos_vector = np.random.standard_normal(EMB_DIM)\n",
    "embedding_matrix, word2id = add_new_word(EOS_TOKEN, eos_vector, EOS_INDEX,\n",
    "                                        embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for context-dependent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2 #define the size of the context window\n",
    "def get_window_int_data(tagged_words, word2id, tag2id):\n",
    "    \"\"\"\n",
    "    Replaces all words and tags with their corresponding ids and\n",
    "    generates an array of label ids Y and the training data set X, which\n",
    "    consists of arrays of word indexes (of tagged word and its context).\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    unk_count = 0\n",
    "    \n",
    "    span = 2*CONTEXT_SIZE + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    padding = [(EOS_TOKEN, None)]*CONTEXT_SIZE\n",
    "    buffer += padding + tagged_words[:CONTEXT_SIZE]\n",
    "    \n",
    "    for item in (tagged_words[CONTEXT_SIZE:] + padding):\n",
    "        buffer.append(item)\n",
    "        \n",
    "        #the input to the model is the ids of all words in the window\n",
    "        window_ids = np.array([word2id.get(word) if (word in word2id) else UNK_INDEX\n",
    "                              for (word, _) in buffer])\n",
    "        \n",
    "        X.append(window_ids)\n",
    "        \n",
    "        #the label is the tag of the middle word\n",
    "        middle_word, middle_tag = buffer[CONTEXT_SIZE]\n",
    "        Y.append(tag2id.get(middle_tag))\n",
    "        \n",
    "        if middle_word not in word2id:\n",
    "            unk_count +=1\n",
    "        print(\"Data created. Percentage of unknown words: %.3f\" % (unk_count/len(tagged_words)))\n",
    "        return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_context_sensitive_model(embedding_matrix, class_count):\n",
    "    \"\"\"\n",
    "    Creates and returns a parts of speech model, which takes as\n",
    "    input a tagged word and its context.\n",
    "    \"\"\"\n",
    "    vocab_length = len(embedding_matrix)\n",
    "    total_span = CONTEXT_SIZE * 2 + 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length,\n",
    "                       output_dim=EMB_DIM,\n",
    "                       weights=[embedding_matrix],\n",
    "                       input_length=total_span)),\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_SIZE))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "                 loss=\"sparse_categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, id2word, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the given model by computing the accuracy of its predictions\n",
    "    on the given test data and prints out 10 most mistagged words.\n",
    "    \"\"\"\n",
    "    _, acc = model.evaluate(x_test, y_test) #get accuracy of model\n",
    "    print(\"Accuracy: %.2f\" % acc)\n",
    "    \n",
    "    #get most commonly mistagged words\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    error_counter = collections.Counter()\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        correct_tag_id = np.argmax(y_test[i])\n",
    "        if y_new[i] != correct_tag_id:\n",
    "            if isinstance(x_test[i], np.ndarray):\n",
    "                word = id2word[x_test[i][CONTEXT_SIZE]]\n",
    "            else:\n",
    "                word = id2word[x_test[i]]\n",
    "            error_counter[word] += 1\n",
    "    \n",
    "    print(\"Most common errors:\\n\", error_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 0.000\n",
      "Data created. Percentage of unknown words: 0.000\n",
      "1/1 [==============================] - 0s 182ms/sample - loss: 4.2616 - acc: 0.0000e+00\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Can not squeeze dim[1], expected a dimension of 1, got 11\n\t [[{{node metrics_2/acc/Squeeze}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-9c81d13a43b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 verbose=1)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs_pos_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-1f507fd8004e>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, id2word, x_test, y_test)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprints\u001b[0m \u001b[0mout\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0mmost\u001b[0m \u001b[0mmistagged\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get accuracy of model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1009\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m           steps=steps)\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m   def predict(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Can not squeeze dim[1], expected a dimension of 1, got 11\n\t [[{{node metrics_2/acc/Squeeze}}]]"
     ]
    }
   ],
   "source": [
    "X_train2, Y_train2 = get_window_int_data(train_words, word2id, tag2id)\n",
    "X_test2, Y_test2 = get_window_int_data(test_words, word2id, tag2id)\n",
    "Y_train2, Y_test2 = to_categorical(Y_train2), to_categorical(Y_test2)\n",
    "\n",
    "cs_pos_model = define_context_sensitive_model(embedding_matrix, len(tag2id))\n",
    "cs_pos_model.fit(X_train2,\n",
    "                Y_train2,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=1,\n",
    "                verbose=1)\n",
    "\n",
    "evaluate_model(cs_pos_model, id2word, X_test2, Y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LDA to extract main topics for R1 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_abstracts_R1 = [doc.split() for doc in processed_abstracts_R1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_abstracts_R1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_abstracts_R1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                    num_topics = 3, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics(num_topics=3, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LDA to extract main topics from R2 abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_abstracts_R2 = [doc.split() for doc in processed_abstracts_R2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_abstracts_R2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_abstracts_R2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                    num_topics = 3, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics(num_topics=8, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_abstracts_R1= [text.lower() for text in cancer_abstracts_R1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip all punctuation from each article\n",
    "# This uses str.translate to map all punctuation to the empty string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "cancer_abstracts_R1 = [text.translate(table) for text in cancer_abstracts_R1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all numbers in the article to the word 'num' using regular expressions\n",
    "cancer_abstracts_R1 = [re.sub(r'\\d+', 'num', text) for text in cancer_abstracts_R1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopwords list, convert to a set for speed\n",
    "stopwords = set(nltk.corpus.stopwords.words('english') + ['reuter', '\\x03'])\n",
    "cancer_abstracts_R1 = [[word for word in text.split() if word not in stopwords] for text in cancer_abstracts_R1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "lmtzr = WordNetLemmatizer()\n",
    "cancer_abstracts_R1 = [\"\".join([lmtzr.lemmatize(word) for word in text]) for text in cancer_abstracts_R1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10)\n",
    "vectorizer.fit(cancer_abstracts_R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bag of words object with maximum vocab size of 1000\n",
    "counter = sklearn.feature_extraction.text.CountVectorizer(max_features = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bag of words model as sparse matrix\n",
    "bag_of_words = counter.fit_transform(cancer_abstracts_R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=counter.get_feature_names(),columns=[\"idf_weights\"]);\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tf-idf object with maximum vocab size of 1000\n",
    "tf_counter = sklearn.feature_extraction.text.TfidfVectorizer(max_features = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tf-idf matrix as sparse matrix\n",
    "tfidf = tf_counter.fit_transform(cancer_abstracts_R1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
